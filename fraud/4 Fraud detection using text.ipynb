{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fraud detection using text\n",
    "In this final chapter, you will use text data, text mining and topic modeling to detect fraudulent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word search with dataframes\n",
    "In this exercise you're going to work with text data, containing emails from Enron employees. The Enron scandal is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is to find all emails that mention specific words, such as \"sell enron stock\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           From                          To  \\\n",
      "154  ('sarah.palmer@enron.com')  ('sarah.palmer@enron.com')   \n",
      "\n",
      "                    Date                                            content  \\\n",
      "154  2002-02-01 14:53:35  \\nJoint Venture: A 1997 Enron Meeting Belies O...   \n",
      "\n",
      "                                         clean_content  \n",
      "154  joint venture enron meeting belies officers cl...  \n"
     ]
    }
   ],
   "source": [
    "# Import module\n",
    "from pandas import read_pickle\n",
    "\n",
    "# Load Data\n",
    "df = read_pickle('data/eron.pkl')\n",
    "\n",
    "# Find all cleaned emails that contain 'sell enron stock'\n",
    "mask = df['clean_content'].str.contains('sell enron stock', na=False)\n",
    "\n",
    "# Select the data from df using the mask\n",
    "print(df.loc[mask])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You see that searching for particular string values in a dataframe can be relatively easy, and allows you to include textual data into your model or analysis. You can use this word search as an additional flag, or as a feauture in your fraud detection model. Let's now have a look at how to filter the data using multiple search terms."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using list of terms\n",
    "Oftentimes you don't want to search on just one term. You probably can create a full \"fraud dictionary\" of terms that could potentially flag fraudulent clients and/or transactions. Fraud analysts often will have an idea what should be in such a dictionary. In this exercise you're going to flag a multitude of terms, and in the next exercise you'll create a new flag variable out of it. The 'flag' can be used either directly in a machine learning model as a feature, or as an additional filter on top of your machine learning model results. Let's first use a list of terms to filter our data on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              From                               To  \\\n",
      "0  ('advdfeedback@investools.com')  ('advdfeedback@investools.com')   \n",
      "1    ('richard.sanders@enron.com')    ('richard.sanders@enron.com')   \n",
      "2            ('m..love@enron.com')            ('m..love@enron.com')   \n",
      "3     ('leslie.milosevich@kp.org')     ('leslie.milosevich@kp.org')   \n",
      "4     ('rtwait@graphicaljazz.com')     ('rtwait@graphicaljazz.com')   \n",
      "\n",
      "                  Date                                            content  \\\n",
      "0  2002-01-29 23:20:55  INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
      "1  2000-09-20 19:07:00  ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
      "2  2001-10-30 16:15:17  hey you are not wearing your target purple shi...   \n",
      "3  2002-01-30 17:54:18  Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...   \n",
      "4  2002-01-30 19:36:01  Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...   \n",
      "\n",
      "                                       clean_content  \n",
      "0  investools advisory free digest trusted invest...  \n",
      "1  forwarded richard b sanders hou ect pm justin ...  \n",
      "2  hey wearing target purple shirt today mine wan...  \n",
      "3  leslie milosevich santa clara avenue alameda c...  \n",
      "4  rini twait e th ave longmont co rtwait graphic...  \n"
     ]
    }
   ],
   "source": [
    "# Create a list of terms to search for\n",
    "searchfor = ['enron stock', 'sell stock', 'stock bonus', 'sell enron stock']\n",
    "\n",
    "# Filter cleaned emails on searchfor list and select from df\n",
    "filtered_emails = df.loc[df['clean_content'].str.contains('|'.join(searchfor), na=False)]\n",
    "print(filtered_emails.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By joining the search terms with the 'or' sign, i.e. |, you can search on a multitude of terms in your dataset very easily. Let's now create a flag from this which you can use as a feature in a machine learning model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a flag\n",
    "This time you are going to create an actual flag variable that gives a 1 when the emails get a hit on the search terms of interest, and 0 otherwise. This is the last step you need to make in order to actually use the text data content as a feature in a machine learning model, or as an actual flag on top of model results. You can continue working with the dataframe df containing the emails, and the searchfor list is the one defined in the last exercise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1776\n",
      "1     314\n",
      "Name: flag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create flag variable where the emails match the searchfor terms\n",
    "df['flag'] = np.where((df['clean_content'].str.contains('|'.join(searchfor)) == True), 1, 0)\n",
    "\n",
    "# Count the values of the flag variable\n",
    "count = df['flag'].value_counts()\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You have now managed to search for a list of strings in several lines of text data. These skills come in handy when you want to flag certain words based on what you discovered in your topic model, or when you know beforehand what you want to search for. In the next exercises you're going to learn how to clean text data and to create your own topic model to further look for indications of fraud in your text data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing stopwords\n",
    "In the following exercises you're going to clean the Enron emails, in order to be able to use the data in a topic model. Text cleaning can be challenging, so you'll learn some steps to do this well. The dataframe containing the emails df is available. In a first step you need to define the list of stopwords and punctuations that are to be removed in the next exercise from the text data. Let's give it a try."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Import nltk packages and string\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Define stopwords to exclude\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\", \"ect\", \"u\", \"fwd\", \"www\", \"com\"))\n",
    "\n",
    "# Define punctuations to exclude and lemmatizer\n",
    "exclude = set(string.punctuation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning text data\n",
    "Now that you've defined the stopwords and punctuations, let's use these to clean our enron emails in the dataframe df further. The lists containing stopwords and punctuations are available under stop and exclude There are a few more steps to take before you have cleaned data, such as \"lemmatization\" of words, and stemming the verbs. The verbs in the email data are already stemmed, and the lemmatization is already done for you in this exercise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alejandro.robles/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investools', 'advisory', 'free', 'digest', 'trusted', 'investment', 'advice', 'unsubscribe', 'free', 'newsletter', 'please', 'see', 'issue', 'fried', 'sell', 'stock', 'gain', 'month', 'km', 'rowe', 'january', 'index', 'confirms', 'bull', 'market', 'aloy', 'small', 'cap', 'advisor', 'earns', 'lbix', 'compounding', 'return', 'pine', 'tree', 'pcl', 'undervalued', 'high', 'yield', 'bank', 'put', 'customer', 'first', 'aso', 'word', 'sponsor', 'top', 'wall', 'street', 'watcher', 'ben', 'zacks', 'year', 'year', 'gain', 'moving', 'best', 'brightest', 'wall', 'street', 'big', 'money', 'machine', 'earned', 'ben', 'zacks', 'five', 'year', 'average', 'annual', 'gain', 'start', 'outperforming', 'long', 'term', 'get', 'zacks', 'latest', 'stock', 'buylist', 'free', 'day', 'trial', 'investools', 'c', 'go', 'zaks', 'mtxtu', 'zakstb', 'investools', 'advisory', 'john', 'brobst', 'investools', 'fried', 'sell', 'stock', 'lock', 'month', 'km', 'david', 'fried', 'know', 'stock', 'undervalued', 'company', 'management', 'buy', 'back', 'share', 'open', 'market', 'latest', 'triumph', 'pocketing', 'impressive', 'gain', 'three', 'short', 'month', 'selling', 'four', 'buyback', 'stock', 'include', 'gain', 'auto', 'retailer', 'automation', 'incorporated', 'gain', 'digital', 'phone', 'system', 'purveyor', 'inter', 'tel', 'intl', 'fried', 'recent', 'move', 'buy', 'kmart', 'corporation', 'km', 'beleaguered', 'discount', 'retailer', 'declared', 'bankruptcy', 'think', 'k', 'mart', 'go', 'business', 'fried', 'say', 'take', 'recovery', 'possibility', 'bought', 'share', 'another', 'fried', 'pick', 'c', 'cor', 'net', 'corporation', 'ccbl', 'provides', 'range', 'technology', 'service', 'broadband', 'network', 'today', 'telecom', 'spending', 'slowdown', 'hit', 'company', 'hard', 'net', 'sale', 'fell', 'million', 'last', 'quarter', 'caused', 'net', 'loss', 'million', 'v', 'million', 'gain', 'last', 'year', 'fried', 'cite', 'buyback', 'plan', 'million', 'restructuring', 'charge', 'proof', 'management', 'see', 'rosier', 'future', 'david', 'fried', 'advice', 'see', 'buyback', 'index', 'portfolio', 'january', 'buyback', 'letter', 'david', 'fried', 'provides', 'wealth', 'building', 'opportunity', 'company', 'repurchasing', 'stock', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'back', 'mtxtu', 'back', 'rowe', 'january', 'index', 'confirms', 'bull', 'market', 'aloy', 'rowe', 'say', 'january', 'index', 'confirms', 'see', 'bull', 'market', 'first', 'five', 'trading', 'day', 'provided', 'gain', 'nasdaq', 'p', 'dow', 'industrials', 'rowe', 'say', 'five', 'day', 'index', 'correctly', 'predicted', 'market', 'direction', 'year', 'since', 'four', 'exception', 'include', 'three', 'war', 'year', 'fed', 'fund', 'rate', 'doubled', 'year', 'rowe', 'maintains', 'sure', 'recommendation', 'seven', 'company', 'say', 'leading', 'market', 'one', 'alloy', 'incorporated', 'aloy', 'medium', 'company', 'direct', 'marketer', 'provides', 'content', 'community', 'commerce', 'generation', 'roughly', 'million', 'people', 'year', 'age', 'rowe', 'like', 'market', 'account', 'billion', 'disposable', 'income', 'grow', 'faster', 'overall', 'population', 'q', 'saw', 'earnings', 'increase', 'sale', 'another', 'rowe', 'pick', 'new', 'century', 'financial', 'corporation', 'ncen', 'financier', 'make', 'buy', 'sell', 'service', 'sub', 'prime', 'mortgage', 'loan', 'secured', 'first', 'mortgage', 'single', 'family', 'home', 'borrower', 'typically', 'plenty', 'equity', 'property', 'secure', 'loan', 'suffer', 'weak', 'credit', 'profile', 'high', 'debt', 'income', 'ratio', 'q', 'earnings', 'grew', 'hike', 'sale', 'rowe', 'advice', 'see', 'investment', 'opportunity', 'february', 'wall', 'street', 'digest', 'momentum', 'investor', 'donald', 'rowe', 'target', 'stock', 'mutual', 'fund', 'capable', 'generating', 'annual', 'return', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'wall', 'mtxtu', 'wall', 'small', 'cap', 'advisor', 'earns', 'lbix', 'major', 'index', 'suffered', 'terrible', 'year', 'richard', 'geist', 'recommendation', 'earned', 'healthy', 'list', 'many', 'reason', 'selection', 'see', 'growth', 'going', 'forward', 'include', 'extremely', 'bullish', 'monetary', 'condition', 'high', 'productivity', 'inflation', 'sight', 'yield', 'curve', 'continues', 'steepen', 'also', 'investor', 'sentiment', 'poll', 'becoming', 'bearish', 'always', 'contrary', 'indicator', 'say', 'geist', 'latest', 'recommendation', 'buy', 'share', 'leading', 'brand', 'lbix', 'company', 'canada', 'largest', 'independent', 'food', 'brand', 'management', 'company', 'expanding', 'u', 'geist', 'particularly', 'like', 'firm', 'save', 'money', 'integrated', 'distribution', 'system', 'system', 'make', 'product', 'raw', 'material', 'provides', 'packaging', 'warehousing', 'distribution', 'recent', 'financial', 'result', 'show', 'leading', 'brand', 'roll', 'fy', 'saw', 'revenue', 'grow', 'million', 'net', 'income', 'million', 'per', 'share', 'last', 'year', 'loss', 'geist', 'predicts', 'company', 'see', 'revenue', 'reach', 'million', 'million', 'yield', 'forward', 'pe', 'think', 'lbix', 'significantly', 'undervalued', 'geist', 'say', 'range', 'leading', 'brand', 'strong', 'buy', 'richard', 'geist', 'advice', 'see', 'highlighted', 'stock', 'february', 'richard', 'geist', 'strategic', 'investing', 'richard', 'geist', 'integrates', 'psychological', 'aspect', 'investing', 'methodology', 'selecting', 'small', 'company', 'stock', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'stin', 'mtxtu', 'stin', 'compounding', 'return', 'pine', 'tree', 'pcl', 'growing', 'tree', 'usually', 'noisy', 'business', 'catch', 'attention', 'investment', 'medium', 'good', 'business', 'say', 'dick', 'young', 'timber', 'business', 'le', 'volatile', 'capital', 'intensive', 'manufacturing', 'young', 'see', 'demand', 'timber', 'increasing', 'population', 'increase', 'note', 'average', 'return', 'timber', 'investment', 'outperformed', 'p', 'average', 'annual', 'return', 'young', 'favorite', 'timber', 'play', 'plum', 'creek', 'timber', 'pcl', 'one', 'largest', 'private', 'timberland', 'owner', 'u', 'reit', 'primary', 'goal', 'profit', 'acquiring', 'managing', 'land', 'young', 'say', 'plum', 'creek', 'timber', 'yield', 'status', 'reit', 'make', 'ideal', 'tax', 'deferred', 'account', 'another', 'young', 'timber', 'selection', 'deltic', 'timber', 'corporation', 'del', 'company', 'grows', 'harvest', 'timber', 'acre', 'arkansas', 'louisiana', 'main', 'company', 'goal', 'expand', 'timber', 'holding', 'sustainable', 'harvest', 'level', 'young', 'say', 'share', 'good', 'portfolio', 'counterweight', 'value', 'investor', 'appreciate', 'intrinsic', 'worth', 'underlying', 'real', 'natural', 'resource', 'dick', 'young', 'advice', 'see', 'investment', 'commentary', 'february', 'richard', 'young', 'intelligence', 'report', 'richard', 'young', 'us', 'buy', 'hold', 'strategy', 'mentor', 'warren', 'buffett', 'uncover', 'low', 'risk', 'high', 'reward', 'opportunity', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'inte', 'mtxtu', 'inte', 'undervalued', 'high', 'yield', 'bank', 'put', 'customer', 'first', 'aso', 'amsouth', 'bancorp', 'aso', 'giving', 'investor', 'healthy', 'yield', 'risk', 'involved', 'say', 'jodie', 'wei', 'investment', 'quality', 'trend', 'billion', 'asset', 'amsouth', 'one', 'largest', 'financial', 'institution', 'south', 'office', 'credit', 'bank', 'success', 'putting', 'customer', 'first', 'wei', 'like', 'amsouth', 'us', 'new', 'technology', 'save', 'money', 'streamlining', 'operation', 'note', 'amsouth', 'ranked', 'number', 'six', 'eweek', 'fast', 'track', 'list', 'company', 'deploy', 'cutting', 'edge', 'technology', 'throughout', 'operation', 'number', 'merrill', 'lynch', 'financial', 'service', 'firm', 'placed', 'higher', 'also', 'amsouth', 'internet', 'banking', 'group', 'quadrupled', 'customer', 'base', 'last', 'year', 'wei', 'say', 'aso', 'share', 'undervalued', 'stock', 'selling', 'near', 'yield', 'wei', 'see', 'upside', 'potential', 'dividend', 'risen', 'annually', 'past', 'year', 'buyback', 'plan', 'million', 'share', 'authorized', 'september', 'stock', 'pe', 'reasonable', 'x', 'priced', 'yield', 'aso', 'undervalued', 'buy', 'considered', 'wei', 'say', 'jodie', 'wei', 'advice', 'see', 'investment', 'spotlight', 'january', 'income', 'digest', 'digest', 'excerpt', 'investment', 'publication', 'highlight', 'weather', 'income', 'oriented', 'opportunity', 'uncovered', 'top', 'mind', 'wall', 'street', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'indi', 'mtxtu', 'indi', 'word', 'sponsor', 'new', 'report', 'top', 'pick', 'despite', 'slumping', 'economy', 'shaky', 'stock', 'market', 'frank', 'curzio', 'bull', 'eye', 'pick', 'gained', 'whopping', 'curzio', 'selected', 'stock', 'incredible', 'potential', 'get', 'red', 'hot', 'pick', 'today', 'click', 'investools', 'c', 'go', 'fxcp', 'fxcp', 'mtxtu', 'disclaimer', 'investools', 'advisory', 'published', 'solely', 'informational', 'purpose', 'solicit', 'offer', 'buy', 'sell', 'stock', 'mutual', 'fund', 'security', 'attempt', 'claim', 'complete', 'description', 'security', 'market', 'development', 'referred', 'material', 'expression', 'opinion', 'change', 'without', 'notice', 'information', 'obtained', 'internal', 'external', 'source', 'investools', 'considers', 'reliable', 'investools', 'independently', 'verified', 'information', 'investools', 'guarantee', 'accurate', 'complete', 'investools', 'undertake', 'advise', 'anyone', 'investools', 'employee', 'officer', 'director', 'may', 'time', 'time', 'position', 'security', 'mentioned', 'may', 'sell', 'buy', 'security', 'remove', 'free', 'email', 'list', 'removed', 'email', 'distribution', 'list', 'free', 'investools', 'advisory', 'update', 'simply', 'click', 'link', 'hit', 'send', 'email', 'launched', 'copy', 'paste', 'email', 'address', 'new', 'outgoing', 'email', 'message', 'hit', 'send', 'email', 'launched', 'mailto', 'bonnie', 'investools', 'important', 'automated', 'system', 'cancel', 'paid', 'newsletter', 'service', 'subscription', 'investools', 'tried', 'unsubscribing', 'past', 'believe', 'received', 'message', 'error', 'please', 'send', 'email', 'mailto', 'itfeedback', 'investools', 'voice', 'concern', 'removed', 'list', 'paid', 'subscription', 'information', 'question', 'investools', 'service', 'paid', 'subscription', 'contact', 'investools', 'customer', 'service', 'center', 'investools', 'cgi', 'bin', 'help', 'pl', 'info', 'pr', 'faq', 'html']\n"
     ]
    }
   ],
   "source": [
    "# Import the lemmatizer from nltk\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Define word cleaning function\n",
    "def clean(text, stop):\n",
    "    text = text.rstrip()\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(i for i in stop_free if i not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(i) for i in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Clean the emails in df and print results\n",
    "text_clean=[]\n",
    "for text in df['clean_content'].astype(str):\n",
    "    text_clean.append(clean(text, stop).split())\n",
    "print(text_clean[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you have cleaned your data entirely with the necessary steps, including splitting the text into words, removing stopwords and punctuations, and lemmatizing your words. You are now ready to run a topic model on this data. In the following exercises you're going to explore how to do that."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create dictionary and corpus\n",
    "In order to run an LDA topic model, you first need to define your dictionary and corpus first, as those need to go into the model. You're going to continue working on the cleaned text data that you've done in the previous exercises. That means that text_clean is available for you already to continue working with, and you'll use that to create your dictionary and corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Define the dictionary\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "\n",
    "# Define the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_clean]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the two ingredients you need to run your topic model on the enron emails. You are now ready for the final step and create your first fraud detection topic model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LDA model\n",
    "Now it's time to build the LDA model. Using the dictionary and corpus, you are ready to discover which topics are present in the Enron emails. With a quick print of words assigned to the topics, you can do a first exploration about whether there are any obvious topics that jump out. Be mindful that the topic model is heavy to calculate so it will take a while to run. Let's give it a try!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"enron\" + 0.011*\"company\" + 0.006*\"employee\" + 0.006*\"stock\" + 0.005*\"energy\"')\n",
      "(1, '0.048*\"td\" + 0.034*\"net\" + 0.033*\"money\" + 0.031*\"tr\" + 0.028*\"width\"')\n",
      "(2, '0.037*\"image\" + 0.009*\"se\" + 0.009*\"ne\" + 0.009*\"sp\" + 0.008*\"wscc\"')\n",
      "(3, '0.044*\"enron\" + 0.010*\"hou\" + 0.007*\"pm\" + 0.007*\"please\" + 0.007*\"development\"')\n",
      "(4, '0.014*\"message\" + 0.010*\"original\" + 0.009*\"e\" + 0.008*\"mail\" + 0.007*\"know\"')\n"
     ]
    }
   ],
   "source": [
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=5)\n",
    "\n",
    "# Save the topics and top 5 words\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "\n",
    "# Print the results\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You have now successfully created your first topic model on the Enron email data. However, the print of words doesn't really give you enough information to find a topic that might lead you to signs of fraud. You'll therefore need to closely inspect the model results in order to be able to detect anything that can be related to fraud in your data. You'll learn more about this in the next video."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding fraudsters based on topic\n",
    "In this exercise you're going to link the results from the topic model back to your original data. You now learned that you want to flag everything related to topic 3. As you will see, this is actually not that straightforward. You'll be given the function get_topic_details() which takes the arguments ldamodel and corpus. It retrieves the details of the topics for each line of text. With that function, you can append the results back to your original data. If you want to learn more detail on how to work with the model results, which is beyond the scope of this course, you're highly encouraged to read this article."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                topic_details_df = topic_details_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score', 'Topic_Keywords']\n",
    "    return topic_details_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dominant_Topic  % Score                                     Topic_Keywords\n",
      "0             0.0   0.9714  enron, company, employee, stock, energy, new, ...\n",
      "1             3.0   0.4260  enron, hou, pm, please, development, corp, mes...\n",
      "2             3.0   0.6837  enron, hou, pm, please, development, corp, mes...\n",
      "3             0.0   0.9935  enron, company, employee, stock, energy, new, ...\n",
      "4             0.0   0.9934  enron, company, employee, stock, energy, new, ...\n",
      "   Dominant_Topic  % Score                                     Topic_Keywords  \\\n",
      "0             0.0   0.9714  enron, company, employee, stock, energy, new, ...   \n",
      "1             3.0   0.4260  enron, hou, pm, please, development, corp, mes...   \n",
      "2             3.0   0.6837  enron, hou, pm, please, development, corp, mes...   \n",
      "3             0.0   0.9935  enron, company, employee, stock, energy, new, ...   \n",
      "4             0.0   0.9934  enron, company, employee, stock, energy, new, ...   \n",
      "\n",
      "                                       Original text  \n",
      "0  [investools, advisory, free, digest, trusted, ...  \n",
      "1  [forwarded, richard, b, sander, hou, pm, justi...  \n",
      "2  [hey, wearing, target, purple, shirt, today, m...  \n",
      "3  [leslie, milosevich, santa, clara, avenue, ala...  \n",
      "4  [rini, twait, e, th, ave, longmont, co, rtwait...  \n",
      "   Dominant_Topic  % Score                                     Topic_Keywords  \\\n",
      "0             0.0   0.9714  enron, company, employee, stock, energy, new, ...   \n",
      "1             3.0   0.4260  enron, hou, pm, please, development, corp, mes...   \n",
      "2             3.0   0.6837  enron, hou, pm, please, development, corp, mes...   \n",
      "3             0.0   0.9935  enron, company, employee, stock, energy, new, ...   \n",
      "4             0.0   0.9934  enron, company, employee, stock, energy, new, ...   \n",
      "\n",
      "                                       Original text  flag  \n",
      "0  [investools, advisory, free, digest, trusted, ...     0  \n",
      "1  [forwarded, richard, b, sander, hou, pm, justi...     1  \n",
      "2  [hey, wearing, target, purple, shirt, today, m...     1  \n",
      "3  [leslie, milosevich, santa, clara, avenue, ala...     0  \n",
      "4  [rini, twait, e, th, ave, longmont, co, rtwait...     0  \n"
     ]
    }
   ],
   "source": [
    "# Run get_topic_details function and check the results\n",
    "print(get_topic_details(ldamodel, corpus).head())\n",
    "\n",
    "# Add original text to topic details in a dataframe\n",
    "contents = pd.DataFrame({'Original text': text_clean})\n",
    "topic_details = pd.concat([get_topic_details(ldamodel, corpus), contents], axis=1)\n",
    "print(topic_details.head())\n",
    "\n",
    "# Create flag for text highest associated with topic 3\n",
    "topic_details['flag'] = np.where((topic_details['Dominant_Topic'] == 3.0), 1, 0)\n",
    "print(topic_details.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You have now flagged all data that is highest associated with topic 3, that seems to cover internal conversation about enron stock options. You are a true detective. With these exercises you have demonstrated that text mining and topic modeling can be a powerful tool for fraud detection.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}